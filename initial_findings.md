
i) A 200-300 word explanation of the expected performance of the model in terms of mean squared error and the key features driving the team’s modeling performance.

The Mean Square Error is as expected. The data has had only preliminary preprocessing and no other data sources have currently been used in this first round, but the data sources originally given. There were only a small amount of missing data points in the feature data. Even though there was a small amount of missing data points, we noticed that if we dropped the rows with missing data points, the mse gets larger and away from the 4 million value. This concluded that dropping those values is not ideal, and we will be exploring other options. When exploring the mean and median, we noticed that the median was used to fill these, and was considered better than the mean when it came to the Mean Square Error value. This is bceause there are numerous outlier data points. At this stage we have chosen to be mindful of these outliers, and in the next round we will consider trimming those missing data points. We predict that by triming them, the Mean Square Error will improve. In terms of the the model performance, the performance is being influenced mainly by the feature sqrt_ft, followed by the number of bathrooms, floor number, and the distance from times square. 

(ii) A 200-300 word summary outlining the team’s intended strategy to improve the predictions
for the final round.

In regards to our strategy for the final round, we've came to the conclusion that one of main things we will be doing is adding more data from outside sources. We want to find data sources that correlate with our current data sources, in which we will be able to create new features that will reduce our Mean Square Error even further, and allow us to explore more of our data. We precisely want to find and develop features that would be in better correlation with rent prices. Another manin thing that we would like to do is the cleaning of more of our data. We do have data gaps regarding rows that are missing certain data points. In addition to imputing the rows where data points are missing, we might have to do manual research for some rows. Athough it's only a small number of rows, we want to make sure that we are taking advantage of all of our data points. The more data, a better number we will get that is below the Mean Square Error. In addition, we will also be using ensemble methods. We will be using multiple algorithms at once, in addition to cross validating to fine tune the hyperparameters.
